# -*- coding: utf-8 -*-
"""AIPredictionStock

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aO6V849-ALpZjgfY9EKFiOwLtXgySdZB

'''

AI Prediction Stock

Product Owner:

Yudong Guan      ID:20111086

Scrum Master:
Ranger Ng        ID:20124370

Development team:

YungHsiang Chang ID:20120197

ChengLin Yi      ID:20121904

Kejian Xie       ID:18026016

'''
"""

#
from google.colab import drive
drive.mount('/content/drive')

#!pip install yahoo_fin

# Import the following libraries:
import os
import time
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from collections import deque
from sklearn import preprocessing
from yahoo_fin import stock_info as si
from tensorflow.keras.models import Sequential
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report

# from yahoo_fin.stock_info import get_data
# # Fetch historical data for Apple
# apple_data = get_data('NVDA')
# print(apple_data)

# set seed, so we can get the same results after rerunning several times
np.random.seed(314)
tf.random.set_seed(314)
random.seed(314)

#Preparing the Dataset
#Downloads the dataset from the Internet and preprocess it.

def shuffle_in_unison(a, b):
    # shuffle two arrays in the same way
    state = np.random.get_state()
    np.random.shuffle(a)
    np.random.set_state(state)
    np.random.shuffle(b)
"""

The n_steps integer indicates the length of the historical sequence we want to use. For instance, choosing 50 means
we will use 50 days of stock prices to predict the next lookup time step.

The ticker argument is the ticker we want to load. Such as, use 'TSLA' for the Tesla stock market, and 'AMZN' for Amazon.

The scale variable indicates whether to scale prices from 0 to 1. We will set this to True, as scaling high values from
0 to 1 will help improve neural network efficiency.

The lookup_step parameter specifies the future lookup step to predict, with the default being 1 (e.g., the next day). For instance, 5 means predicting the next 5 days, and so on.

The split_by_date parameter indicates whether to split the training set and test set by date. If set to False, it uses the train_test_split()
function from sklearn to randomly split the data into training and testing sets. Otherwise, it splits the data in date order.

In this case, we will use all the features available in this dataset: open, high, low, volume, and adjusted close.

"""
def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,
                test_size=0.3, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):

    # see if ticker is already a loaded stock from yahoo finance
    if isinstance(ticker, str):
        # load it from yahoo_fin library, get data
        df = si.get_data(ticker)
    elif isinstance(ticker, pd.DataFrame):
        # already loaded, use it directly
        df = ticker
    else:
        raise TypeError("ticker can be either a str or a `pd.DataFrame` instances")
    # this will contain all the elements we want to return from this function
    result = {}
    # we will also return the original dataframe itself
    result['df'] = df.copy()
    # make sure that the passed feature_columns exist in the dataframe
    for col in feature_columns:
        assert col in df.columns, f"'{col}' does not exist in the dataframe."

    # add date as a column
    if "date" not in df.columns:
        df["date"] = df.index
    if scale:
        column_scaler = {}

        # scale the data prices from 0 to 1
        for column in feature_columns:
            scaler = preprocessing.MinMaxScaler()
            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))
            column_scaler[column] = scaler

        # add the MinMaxScaler instances to the result returned
        result["column_scaler"] = column_scaler

    # add the target column label by shifting by `lookup_step`
    df['future'] = df['adjclose'].shift(-lookup_step)
    # last `lookup_step` columns contains NaN in future column
    # get them before droping NaNs
    last_sequence = np.array(df[feature_columns].tail(lookup_step))

    # drop NaNs
    df.dropna(inplace=True)
    sequence_data = []
    sequences = deque(maxlen=n_steps)

    for entry, target in zip(df[feature_columns + ["date"]].values, df['future'].values):
        sequences.append(entry)
        if len(sequences) == n_steps:
            sequence_data.append([np.array(sequences), target])

    # Get the last sequence by appending the last `n_steps` sequence with `lookup_step` sequence.
    # For instance, if `n_steps`=50 and `lookup_step`=10, `last_sequence` should be of length 50 + 10=60.
    # This `last_sequence` will be used to predict future stock prices that are not available in the dataset.
    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)
    last_sequence = np.array(last_sequence).astype(np.float32)

    # add to result
    result['last_sequence'] = last_sequence

    # construct the X's and y's
    X, y = [], []

    for seq, target in sequence_data:
        X.append(seq)
        y.append(target)

    # convert to numpy arrays
    X = np.array(X)
    y = np.array(y)
    if split_by_date:
        # split the dataset into training & testing sets by date (not randomly splitting)
        train_samples = int((1 - test_size) * len(X))
        result["X_train"] = X[:train_samples]
        result["y_train"] = y[:train_samples]
        result["X_test"]  = X[train_samples:]
        result["y_test"]  = y[train_samples:]
        aX_test = X[:train_samples]
        if shuffle:
            # shuffle the datasets for training (if shuffle parameter is set)
            shuffle_in_unison(result["X_train"], result["y_train"])
            shuffle_in_unison(result["X_test"], result["y_test"])
    else:
        # split the dataset randomly
        result["X_train"], result["X_test"], result["y_train"], result["y_test"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)
    # get the list of test set dates
    dates = result["X_test"][:, -1, -1]

    # retrieve test features from the original dataframe
    result["test_df"] = result["df"].loc[dates]

    # remove duplicated dates in the testing dataframe
    result["test_df"] = result["test_df"][~result["test_df"].index.duplicated(keep='first')]

    # remove dates from the training/testing sets & convert to float32
    result["X_train"] = result["X_train"][:, :, :len(feature_columns)].astype(np.float32)
    result["X_test"] = result["X_test"][:, :, :len(feature_columns)].astype(np.float32)
    return result

# Model Creation the core function to build a model
"""
The built model constructs a Recurrent Neural Network (RNN) with a dense layer serving as the output layer containing a single neuron. This model expects
a sequence of features with a sequence_length (in this case, set to 50 or 100) consecutive time steps (representing days in this dataset) and produces a
single value indicating the price of the next time step.

It also accepts n_features as an argument, which includes the 'adjclose', 'open', 'high', 'low', and 'volume' columns, totaling 5 features.

The default parameters can be adjusted according to requirements. n_layers determines the number of RNN layers, dropout represents the dropout rate after
each RNN layer, and units specifies the number of RNN units.

"""

# 256 LSTM neurons, Number of cell units
def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.2,
                loss="mean_absolute_error", optimizer="rmsprop", bidirectional=False):
    model = Sequential()
    for i in range(n_layers):
        if i == 0:
            # first layer
            if bidirectional:
                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))
            else:
                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))
        elif i == n_layers - 1:
            # last layer
            if bidirectional:
                model.add(Bidirectional(cell(units, return_sequences=False)))
            else:
                model.add(cell(units, return_sequences=False))
        else:
            # hidden layers
            if bidirectional:
                model.add(Bidirectional(cell(units, return_sequences=True)))
            else:
                model.add(cell(units, return_sequences=True))
        # add dropout after each layer
        # Dropout is to randomly discard the neurons in the previous layer, which helps to solve the problem of overfitting.
        # 20% dropout
        model.add(Dropout(0.2))
        model.add(Dense(1, activation="linear"))

        # compile the model
        model.compile(loss=loss, metrics=["mean_absolute_error"], optimizer=optimizer)
        #model.compile(loss=loss, metrics=["mean_absolute_error",f1_m,precision_m, recall_m], optimizer=optimizer)
    return model

# Training the model and initialize all parameters
# Window size or the sequence length
N_STEPS = 50

# Lookup step, 1 is the next day
LOOKUP_STEP = 15

# whether to scale feature columns & output price as well
SCALE = True
scale_str = f"sc-{int(SCALE)}"

# whether to shuffle the dataset
SHUFFLE = True
shuffle_str = f"sh-{int(SHUFFLE)}"

# whether to split the training/testing set by date
SPLIT_BY_DATE = False
split_by_date_str = f"sbd-{int(SPLIT_BY_DATE)}"

# features to use to predict the next price value
FEATURE_COLUMNS = ["adjclose", "volume", "open", "high", "low"]

# date now
date_now = time.strftime("%Y-%m-%d")

# RNN cell to use, default is LSTM
CELL = LSTM

# whether to use bidirectional recurrent neural networks
BIDIRECTIONAL = False

# Training parameters
# mean absolute error (mae)loss
# mean absolute error (mae) metric
# Loss function to use for this regression problem include Mean Squared Error (MSE) as well.
LOSS ='mse'

# Optimization algorithm to use, defaulting to Adam
OPTIMIZER = "adam"

# Amazon stock market
ticker = "AMZN"
ticker_data_filename = os.path.join("data", f"{ticker}_{date_now}.csv")

# model name to save, making it as unique as possible based on parameters
model_name = f"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\
{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{2}-units-{256}"
if BIDIRECTIONAL:
    model_name += "-b"

# make sure the results, logs, and data folders exist before we train
# create these folders if they does not exist
if not os.path.isdir("results"):
    os.mkdir("results")
if not os.path.isdir("logs"):
    os.mkdir("logs")
if not os.path.isdir("data"):
    os.mkdir("data")

# load the dataset

data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE,
                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=0.3,
                feature_columns=FEATURE_COLUMNS)

# save the dataframe  AMZN_2022_08_14.csv
data["df"].to_csv(ticker_data_filename)

df = pd.read_csv(ticker_data_filename)

df.head()



#Check for missing or Null Data points

df.isnull().sum()
df.isna().sum()

# show summary statistics

print(df.describe())

#Set the input (X) and output/target (y)

X = df.iloc[:, 1:7].values
y = df.iloc[:, 7].values
print(X)

# Handling Target Categorical Value via Encoding

labelencoder_y = LabelEncoder()
y = labelencoder_y.fit_transform(y)

# Reducing computational time by scaling the data efficiently.
# For scaling, we utilize the StandardScaler module from scikit-learn to scale both the training and testing datasets.
# Create StandardScaler object

sc = StandardScaler()
X = sc.fit_transform(X)
class_names=[0,1]

#Its time for splitting the data into training and testing parts
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=3)

#Build the Model and fit to train set
mlp = MLPClassifier(
    max_iter=200,
    alpha=0.1,
    activation='logistic',
    solver='adam')
mlp.fit(X_train, y_train)           #  Fit Model
mlp_predict = mlp.predict(X_test)   #  Make Prediction


print('MLP Accuracy: {:.2f}%'.format(accuracy_score(y_test, mlp_predict) * 100))
print('MLP Classification report:\n\n', classification_report(y_test, mlp_predict))
print('MLP Training set score: {:.2f}%'.format(mlp.score(X_train, y_train) * 100))
print('MLP Testing set score: {:.2f}%'.format(mlp.score(X_test, y_test) * 100))

"""As above, Accuracy score of 100.00%, precision score of 100.00%, recall score of 100.00% and f1-score of 100.00%."""

# Finally, call the above functions to build the model and fit it to the training set to predict the stock price in the future.
"""
We used ModelCheckpoint, which saves our model in each epoch during training.
Additionally, we utilized TensorBoard to visualize the model's performance during the training process.

The model will be trained for 400 epochs, so it will take some time.

"""

# Construct the model
model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=256, cell=CELL, n_layers=2,
                    dropout=Dropout(0.2), optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)

# Some tensorflow callbacks
checkpointer = ModelCheckpoint(os.path.join("results", model_name + ".h5"), save_weights_only=True, save_best_only=True, verbose=1)
tensorboard = TensorBoard(log_dir=os.path.join("logs", model_name))

# Train the model and save the weights whenever a new optimal model is encountered using ModelCheckpoint.
history = model.fit(data["X_train"], data["y_train"],  #  Fit Model
                      # The number of data samples to use on each training iteration
                      batch_size=50,
                      # The number of times the learning algorithm will pass through the entire training dataset, we used 300 here
                      epochs=300,
                      validation_data=(data["X_test"], data["y_test"]),
                      callbacks=[checkpointer, tensorboard],
                      verbose=1)

# Commented out IPython magic to ensure Python compatibility.
# Try running TensorBoard to plot the epoch_loss and epoch_mean_absolute_error after training the model.
# !pip install tensorboardcolab
# %load_ext tensorboard
# %tensorboard --logdir logs



"""As above, the curve is the validation loss. As you can see, it is significantly decreasing over time."""

# Testing the Model
# let's evaluate it and see how it's performing on the testing set. plots the true and predicted prices on the same plot using matplotlib.

import matplotlib.pyplot as plt

def plot_graph(test_df):
    """
    This function plots true close price along with predicted close price
    with blue and red colors respectively
    """
    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')
    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')
    plt.xlabel("Days")
    plt.ylabel("Price")
    plt.legend(["Actual Price", "Predicted Price"])
    plt.show()

"""
Use the function to takes the model and the data returned by the create_model() and load_data() functions, respectively.
Construct a dataframe that includes the predicted adjclose along with the true future adjclose, as well as calculate buy and sell profits.

This function takes the model and data dictionaries to construct a final dataframe that includes the features along with
true and predicted prices of the testing dataset.


"""
def get_final_df(model, data):

    # If the predicted future price is higher than the current price, calculate the difference between the true future price
    # and the current price to determine the buy profit.

    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0

    # if the predicted future price is lower than the current price,
    # then subtract the true future price from the current price
    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0
    X_test = data["X_test"]
    y_test = data["y_test"]

    # perform prediction and get prices
    y_pred = model.predict(X_test)
    if SCALE:
        y_test = np.squeeze(data["column_scaler"]["adjclose"].inverse_transform(np.expand_dims(y_test, axis=0)))
        y_pred = np.squeeze(data["column_scaler"]["adjclose"].inverse_transform(y_pred))
    test_df = data["test_df"]

    # add predicted future prices to the dataframe
    test_df[f"adjclose_{LOOKUP_STEP}"] = y_pred

    # add true future prices to the dataframe
    test_df[f"true_adjclose_{LOOKUP_STEP}"] = y_test

    # sort the dataframe by date
    test_df.sort_index(inplace=True)
    final_df = test_df

    # add the buy profit column
    final_df["buy_profit"] = list(map(buy_profit,
                                    final_df["adjclose"],
                                    final_df[f"adjclose_{LOOKUP_STEP}"],
                                    final_df[f"true_adjclose_{LOOKUP_STEP}"])
                                    # since we don't have profit for last sequence, add 0's
                                    )
    # add the sell profit column
    final_df["sell_profit"] = list(map(sell_profit,
                                    final_df["adjclose"],
                                    final_df[f"adjclose_{LOOKUP_STEP}"],
                                    final_df[f"true_adjclose_{LOOKUP_STEP}"])
                                    # since we don't have profit for last sequence, add 0's
                                    )
    return final_df

# Define a function that is responsible for predicting the next future price
def predict(model, data):
    # retrieve the last sequence from data
    last_sequence = data["last_sequence"][-N_STEPS:]

    # expand dimension
    last_sequence = np.expand_dims(last_sequence, axis=0)

    # get the prediction (scaled from 0 to 1)
    prediction = model.predict(last_sequence)

    # get the price (by inverting the scaling)
    if SCALE:
        predicted_price = data["column_scaler"]["adjclose"].inverse_transform(prediction)[0][0]
    else:
        predicted_price = prediction[0][0]
    return predicted_price

# Now let's load the optimal weights and proceed with evaluation.
# Load optimal model weights from results folder
model_path = os.path.join("results", model_name) + ".h5"
model.load_weights(model_path)

# Calculating loss and mean absolute error using model.evaluate() method and an evaluate the model
loss, mae = model.evaluate(data["X_test"], data["y_test"], verbose=0)
# calculate the mean absolute error (inverse scaling)
if SCALE:
    mean_absolute_error = data["column_scaler"]["adjclose"].inverse_transform([[mae]])[0][0]
else:
    mean_absolute_error = mae

# Call the get_final_df() function we defined earlier to construct our testing set dataframe. and get the final dataframe for the testing set
final_df = get_final_df(model, data)

# Use predict() function to predict the future price

future_price = predict(model, data)

# Compution the accuracy by counting the number of positive profits (in both buy profit and sell profit)
# We calculate the accuracy by counting the number of positive profits
accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)

# Calculating total buy & sell profit
total_buy_profit  = final_df["buy_profit"].sum()
total_sell_profit = final_df["sell_profit"].sum()

# Total profit by adding sell & buy together
total_profit = total_buy_profit + total_sell_profit

# Dividing total profit by number of testing samples (number of trades)
profit_per_trade = total_profit / len(final_df)

# Printing all the previously calculated metrics

print(f"Future price after {LOOKUP_STEP} days is {future_price:.2f}$")
print(f"{LOSS} loss:", loss)
print("Mean Absolute Error:", mean_absolute_error)
print("Accuracy:", accuracy_score)
print("Total buy profit:", total_buy_profit)
print("Total sell profit:", total_sell_profit)
print("Total profit:", total_profit)
print("Profit per trade:", profit_per_trade)

"""As above shows great, the model says after 15 days that the price of AMZN will be $167.34, that's interesting.

Mean absolute error: we get about 1.61 as error, which means, on average, the model predictions are far by over 1.61$ to the true prices; this will vary from ticker to another, As a result, you should only compare your models using this metric when the ticker is stable.

Buy/Sell profit: This is the profit we get if we opened trades on all the testing samples, from on the  get_final_df() function.

Total profit: This is simply the sum of buy and sell profits.

Profit per trade: The total profit divided by the total number of testing samples.

Accuracy: This is the score for the accuracy of our predictions. This calculation is based on positive profits on all trades in the test sample.
"""

# plot true/pred prices graph
plot_graph(final_df)

"""Look as above, Excellent, as you can see, the blue curve is the actual test set, and the red curve is the predicted prices! That the stock price has recently been increasing, as we predicted."""

# print the last 10 rows of our final dataframe
print(final_df.tail(10))

# save the final dataframe to csv-results folder
csv_results_folder = "csv-results"
if not os.path.isdir(csv_results_folder):
    os.mkdir(csv_results_folder)
csv_filename = os.path.join(csv_results_folder, model_name + ".csv")
final_df.to_csv(csv_filename)




"""# Conclusion

If you have a fast GPU environment, you can try tuning the parameters several times and see how the model performance improves, try training on more epochs, say 512 or more, increase or decrease the batch_size and see if it gets better, or use N_STEPS and LOOKUP_STEPS to find which combination works best.
"""